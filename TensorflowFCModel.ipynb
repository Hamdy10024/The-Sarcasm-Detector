{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorflowFCModel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamdy10024/The-Sarcasm-Detector/blob/master/TensorflowFCModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8W3ymMKhi2p2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn.metrics as sk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6Y8GFJAk1pN",
        "colab_type": "code",
        "outputId": "a1b90aa6-a728-4786-bfaa-7801cd75ad68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jzi1HJxVjvFL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def divideFeatureSets(features):\n",
        "    \"\"\"\n",
        "    This method is used to divide the whole feature sets into four parts:\n",
        "    1.  Training input\n",
        "    2.  Training output\n",
        "    3.  Testing input\n",
        "    4.  Testing output\n",
        "\n",
        "    The default split rate is 30% for testing. It can be cahnged by setting\n",
        "    the value for test_size inside the method.\n",
        "    :param features:\n",
        "    :return: train_input, train_output, test_input, test_output.\n",
        "    \"\"\"\n",
        "    test_size = 0.3\n",
        "    testing_size = int(test_size * len(features))\n",
        "\n",
        "    train_input = list(features[:, 0][:-testing_size])\n",
        "    train_output = list(features[:, 1][:-testing_size])\n",
        "    test_input = list(features[:, 0][-testing_size:])\n",
        "    test_output = list(features[:, 1][-testing_size:])\n",
        "\n",
        "    return train_input, train_output, test_input, test_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FNwO2OEVkAFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "root = '/content/drive/My Drive/DeepLearning/'\n",
        "#%cd /content/drive/My Drive/DeepLearning/\n",
        "#%ls\n",
        "#%pwd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AAqjNfj7j5D0",
        "colab_type": "code",
        "outputId": "7d1a365d-bd80-48f5-942b-860335a98fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "featuresets = np.load(root + 'featuresets.npy')\n",
        "\n",
        "# Divide the feature sets into training and testing set.\n",
        "train_input, train_output, test_input, test_output = divideFeatureSets(featuresets)\n",
        "\n",
        "# Define number of nodes in each layer\n",
        "number_nodes_HL1 = 100\n",
        "number_nodes_HL2 = 100\n",
        "number_nodes_HL3 = 100\n",
        "\n",
        "# Define other constants\n",
        "n_classes = 2\n",
        "batch_size = 64\n",
        "number_epochs = 75\n",
        "\n",
        "# Tensorflow place holder for input ad output to the tensorflow graph\n",
        "x = tf.placeholder('float', [None, len(train_input[0])])\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "# Define the layers using dictionaries. Weights and biases are initialized as\n",
        "#  random numbers.\n",
        "with tf.name_scope(\"HiddenLayer1\"):\n",
        "    hidden_1_layer = {'number_of_neurons': number_nodes_HL1,\n",
        "                  'layer_weights': tf.Variable(\n",
        "                      tf.random_normal([len(train_input[0]), number_nodes_HL1])),\n",
        "                  'layer_biases': tf.Variable(tf.random_normal([number_nodes_HL1]))}\n",
        "\n",
        "with tf.name_scope(\"HiddenLayer2\"):\n",
        "    hidden_2_layer = {'number_of_neurons': number_nodes_HL2,\n",
        "                  'layer_weights': tf.Variable(\n",
        "                      tf.random_normal([number_nodes_HL1, number_nodes_HL2])),\n",
        "                  'layer_biases': tf.Variable(tf.random_normal([number_nodes_HL2]))}\n",
        "\n",
        "with tf.name_scope(\"HiddenLayer3\"):\n",
        "    hidden_3_layer = {'number_of_neurons': number_nodes_HL3,\n",
        "                  'layer_weights': tf.Variable(\n",
        "                      tf.random_normal([number_nodes_HL2, number_nodes_HL3])),\n",
        "                  'layer_biases': tf.Variable(tf.random_normal([number_nodes_HL3]))}\n",
        "\n",
        "with tf.name_scope(\"OutputLayer\"):\n",
        "    output_layer = {'number_of_neurons': None,\n",
        "                'layer_weights': tf.Variable(\n",
        "                    tf.random_normal([number_nodes_HL3, n_classes])),\n",
        "                'layer_biases': tf.Variable(tf.random_normal([n_classes])),}\n",
        "\n",
        "merged_summary_op = tf.summary.merge_all()\n",
        "logs_path = root +'logs'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "inQ0gCbJqzpz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neural_network_model(data):\n",
        "    \"\"\"\n",
        "    This method is used to define how the data flows through the neural\n",
        "    network and how inputs and outputs of different layers are calculated,\n",
        "    given the feature vector.\n",
        "    :param data:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # the output of first layer is input*weights + biases\n",
        "    l1 = tf.add(tf.matmul(data, hidden_1_layer['layer_weights']),\n",
        "                hidden_1_layer['layer_biases'])\n",
        "    # Logit\n",
        "    l1 = tf.nn.relu(l1)\n",
        "\n",
        "    # the ouput of second layer is output of first layer *  weights + biases\n",
        "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['layer_weights']), hidden_2_layer['layer_biases'])\n",
        "    # Logit\n",
        "    l2 = tf.nn.relu(l2)\n",
        "\n",
        "    # Similar as previous\n",
        "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['layer_weights']), hidden_3_layer['layer_biases'])\n",
        "    l3 = tf.nn.relu(l3)\n",
        "\n",
        "    # Finally the output is output of last layer * weights + biases. No logit\n",
        "    #  for last layer as this layer's output is not fed to any other layer.\n",
        "    output = tf.matmul(l3, output_layer['layer_weights']) + output_layer['layer_biases']\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iueNdH7Nq0xV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network(x):\n",
        "    \"\"\"\n",
        "    This method is responsible for training the NN model using\n",
        "    backpropagation and tensorflow\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the prediction.\n",
        "    with tf.name_scope(\"model\"):\n",
        "        prediction = neural_network_model(x)\n",
        "\n",
        "    # Get the cost of this prediction, which is passed through softmax and\n",
        "    # then reduced mean is computed to give final cost.\n",
        "    with tf.name_scope(\"Cost\"):\n",
        "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
        "                              (logits=prediction, labels=y))\n",
        "\n",
        "    # The goal of back-propagation is to minimize the cost. Use AdamOptimizer\n",
        "    #  for that.\n",
        "    with tf.name_scope(\"optimizer\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "    tf.summary.scalar(\"COST\", cost)\n",
        "\n",
        "    merged_summary_op = tf.summary.merge_all()\n",
        "\n",
        "    # Time to trigger Tensorflow\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "\n",
        "        summary_writer = tf.summary.FileWriter(logs_path,\n",
        "                                               graph=tf.get_default_graph())\n",
        "\n",
        "        # Train in epochs\n",
        "        for epoch in range(number_epochs):\n",
        "            epoch_loss = 0\n",
        "            i = 0\n",
        "            while i < len(train_input):\n",
        "                start = i\n",
        "                end = i + batch_size\n",
        "\n",
        "                # Divide into batches\n",
        "                batch_x = np.array(train_input[start:end])\n",
        "                batch_y = np.array(train_output[start:end])\n",
        "\n",
        "                # Run tensorflow to train using this batch\n",
        "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
        "                                         feed_dict={x: batch_x, y: batch_y})\n",
        "                summary_writer.add_summary(summary, epoch * batch_size + i)\n",
        "                # Keep aggregating the cost/loss for calculating the loss of\n",
        "                # the whole epoch\n",
        "                epoch_loss += c\n",
        "\n",
        "                # Increment the batch-start index\n",
        "                i += batch_size\n",
        "\n",
        "            print('Epoch', epoch + 1, 'completed out of', number_epochs, 'loss:',\n",
        "                  epoch_loss)\n",
        "\n",
        "        # Specify the correctness criteria. Prediction = actual\n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "\n",
        "        # Caluculate accuracyusing correctness criteria\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "\n",
        "        # Specify the criteria to get predicted classes, with a default of 1.\n",
        "        y_p = tf.argmax(prediction, 1)\n",
        "\n",
        "        # Run the model on test data tooptain predictions\n",
        "        val_accuracy, y_pred = sess.run([accuracy, y_p],\n",
        "                                        feed_dict={x: test_input,\n",
        "                                                   y: test_output})\n",
        "        # Get actual classes\n",
        "        y_true = np.argmax(test_output, 1)\n",
        "\n",
        "        #Calculate f1 score using scikit-learn.\n",
        "        print('F1 Score:', sk.f1_score(y_true, y_pred))\n",
        "\n",
        "        #Print out the confusion matrix\n",
        "        print(sk.confusion_matrix(y_true, y_pred))\n",
        "\n",
        "        print('Accuracy:', accuracy.eval({x: test_input, y: test_output}))\n",
        "\n",
        "        # Save the model for using in future.\n",
        "        saver = tf.train.Saver()\n",
        "        saver.save(sess, os.path.join(root + 'model\\sarcasm_model.ckpt'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YrUaXxa3tWEb",
        "colab_type": "code",
        "outputId": "bef7e389-d8e2-4e16-921b-ee26476753dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1567
        }
      },
      "cell_type": "code",
      "source": [
        "train_neural_network(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-fff6b7e89cf2>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Epoch 1 completed out of 75 loss: 16729.045137882233\n",
            "Epoch 2 completed out of 75 loss: 7076.477667808533\n",
            "Epoch 3 completed out of 75 loss: 5223.219759941101\n",
            "Epoch 4 completed out of 75 loss: 4291.946821808815\n",
            "Epoch 5 completed out of 75 loss: 3697.488922595978\n",
            "Epoch 6 completed out of 75 loss: 3291.1989347934723\n",
            "Epoch 7 completed out of 75 loss: 2977.4996013641357\n",
            "Epoch 8 completed out of 75 loss: 2777.393129348755\n",
            "Epoch 9 completed out of 75 loss: 2558.454804778099\n",
            "Epoch 10 completed out of 75 loss: 2372.0990315675735\n",
            "Epoch 11 completed out of 75 loss: 2196.654583096504\n",
            "Epoch 12 completed out of 75 loss: 2041.6583578586578\n",
            "Epoch 13 completed out of 75 loss: 1979.7422128915787\n",
            "Epoch 14 completed out of 75 loss: 1870.051458120346\n",
            "Epoch 15 completed out of 75 loss: 1735.4953182935715\n",
            "Epoch 16 completed out of 75 loss: 1632.2116078138351\n",
            "Epoch 17 completed out of 75 loss: 1584.483689546585\n",
            "Epoch 18 completed out of 75 loss: 1500.837418794632\n",
            "Epoch 19 completed out of 75 loss: 1453.7122567892075\n",
            "Epoch 20 completed out of 75 loss: 1403.3071240186691\n",
            "Epoch 21 completed out of 75 loss: 1353.623783648014\n",
            "Epoch 22 completed out of 75 loss: 1312.7344757318497\n",
            "Epoch 23 completed out of 75 loss: 1235.4589066505432\n",
            "Epoch 24 completed out of 75 loss: 1226.8562782406807\n",
            "Epoch 25 completed out of 75 loss: 1168.6465632915497\n",
            "Epoch 26 completed out of 75 loss: 1100.801483988762\n",
            "Epoch 27 completed out of 75 loss: 1083.8030900359154\n",
            "Epoch 28 completed out of 75 loss: 1111.519383609295\n",
            "Epoch 29 completed out of 75 loss: 1046.6665772795677\n",
            "Epoch 30 completed out of 75 loss: 1017.0446489453316\n",
            "Epoch 31 completed out of 75 loss: 1012.3471149206161\n",
            "Epoch 32 completed out of 75 loss: 970.1456110477448\n",
            "Epoch 33 completed out of 75 loss: 929.0609570741653\n",
            "Epoch 34 completed out of 75 loss: 905.8939797878265\n",
            "Epoch 35 completed out of 75 loss: 874.2121036052704\n",
            "Epoch 36 completed out of 75 loss: 861.2057574987411\n",
            "Epoch 37 completed out of 75 loss: 856.1955119371414\n",
            "Epoch 38 completed out of 75 loss: 866.7789173126221\n",
            "Epoch 39 completed out of 75 loss: 897.5173102617264\n",
            "Epoch 40 completed out of 75 loss: 845.9401886463165\n",
            "Epoch 41 completed out of 75 loss: 782.5407007336617\n",
            "Epoch 42 completed out of 75 loss: 753.559174656868\n",
            "Epoch 43 completed out of 75 loss: 736.1939041018486\n",
            "Epoch 44 completed out of 75 loss: 719.8871726989746\n",
            "Epoch 45 completed out of 75 loss: 716.608609855175\n",
            "Epoch 46 completed out of 75 loss: 700.7440273165703\n",
            "Epoch 47 completed out of 75 loss: 678.991902410984\n",
            "Epoch 48 completed out of 75 loss: 671.5797026157379\n",
            "Epoch 49 completed out of 75 loss: 663.4848309755325\n",
            "Epoch 50 completed out of 75 loss: 654.503366291523\n",
            "Epoch 51 completed out of 75 loss: 633.0813828706741\n",
            "Epoch 52 completed out of 75 loss: 626.0233026742935\n",
            "Epoch 53 completed out of 75 loss: 616.4938485622406\n",
            "Epoch 54 completed out of 75 loss: 608.6100947856903\n",
            "Epoch 55 completed out of 75 loss: 602.7774604558945\n",
            "Epoch 56 completed out of 75 loss: 604.62553358078\n",
            "Epoch 57 completed out of 75 loss: 595.3671327233315\n",
            "Epoch 58 completed out of 75 loss: 610.2672873139381\n",
            "Epoch 59 completed out of 75 loss: 639.0201871395111\n",
            "Epoch 60 completed out of 75 loss: 619.7338210344315\n",
            "Epoch 61 completed out of 75 loss: 594.2919043898582\n",
            "Epoch 62 completed out of 75 loss: 584.4817111492157\n",
            "Epoch 63 completed out of 75 loss: 578.2797430753708\n",
            "Epoch 64 completed out of 75 loss: 566.9647228717804\n",
            "Epoch 65 completed out of 75 loss: 563.4201861023903\n",
            "Epoch 66 completed out of 75 loss: 536.0083485841751\n",
            "Epoch 67 completed out of 75 loss: 543.2631409168243\n",
            "Epoch 68 completed out of 75 loss: 537.2268592715263\n",
            "Epoch 69 completed out of 75 loss: 530.9380637407303\n",
            "Epoch 70 completed out of 75 loss: 518.3553216457367\n",
            "Epoch 71 completed out of 75 loss: 507.55904936790466\n",
            "Epoch 72 completed out of 75 loss: 499.55350375175476\n",
            "Epoch 73 completed out of 75 loss: 500.22877249121666\n",
            "Epoch 74 completed out of 75 loss: 509.74960350990295\n",
            "Epoch 75 completed out of 75 loss: 510.06639063358307\n",
            "F1 Score: 0.6716417910447762\n",
            "[[1823 5746]\n",
            " [1470 7380]]\n",
            "Accuracy: 0.56050915\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}